## 第一章  深度学习介绍

　　自古以来，发明家都梦想着能够创造有思想的机器人，当电脑问世以后，人们一直想着它们是否可以变得更加智能。如今人工智能成为一个有着无限商业价值和研究价值的领域，人们也看到了人工智能在图像、语音、自然语言上聚是的巨大成功。

　　这一章，主要介绍人工智能及其重要性，以及什么是数据挖掘、机器学习和深度学习，并介绍它们间的联系和区别，帮助大家从零开始进入深度学习这个充满魔力的领域。

## 1.1 人工智能

　　人工智能（Artificial Intelligence），也称为机器智能，是指由人工制造出来的系统所表现的智能，所谓的智能，即指可以观察周围环境并据此做出行动以达到目的。

　　在人工智能的早期，那些对人类智力来说非常困难、但对计算机来说相对简单的问题迅速得到解决，比如，那些可以通过一系列形式化的数学规则来描述的问题。AI的真正挑战在于解决那些对人来说很容易执行、但很难形式化描述的任务，比如，识别人们所说的话或图像中的脸。对于这些问题，我们人类往往可以凭借直觉轻易地解决，因为我们已经在上万年的进化中形成了这些直觉性的能力，但是机器却很难找到实现的方法。

　　长久以来人们一直相信人工智能是存在的，但是却不知道如何实现。以前的科幻电影总会融入人工智能，比如《星球大战》《终结者》《骇客帝国》，等等。电影的渲染使我们总觉得人工智能缺乏真实感，或者总将人工智能和机器人联系在一起。其实我们身边早已实现了一些弱人工智能，只是因为人工智能听起来很神秘，所以我们往往没有意识到。

　　首先，不要一提到人工智能就想到机器人。机器人只是人工智能的一种容器，如果将人工智能比作大脑，那么机器人就好似身体——然而这个身体却不是必需的，比如现在很火的AlphaGo，其背后充满着软件、算法和数据，它下围棋是一种人格化的体现，然而其本身并没有“机器人”这个硬件形式。

　　人工智能的概念很宽泛，现在根据人工智能的实力将它分成三大类：

　　**（1）弱人工智能**

　　弱人工智能是擅长于单个方面的人工智能。比如战胜世界围棋冠军的人工智能AlphaGo，它只会下围棋，如果你让它识别一下猫和狗，它就不知道该怎么做了。我们现在实现在几乎全是弱人工智能。

　　**（2）强人工智能**

　　这是类似人类级别的人工智能：强人工智能是指在各方面都能和人类比肩的人工智能，人类能干的脑力活儿，它都能干。创造强人工智能比创造弱人工智能难得多。我们现在还做不到，Linda Gottfredson教授把智能定义为“一种宽泛的心埋能力，能够进行 思考、汁划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作。” 强人工智能在进行这些操作时应该和人类一样得心应手。

　　**（3）超人工智能**

　　牛津哲学家、知名人工智能思想家Nick Bostrom把超级智能定义为“在几乎所有领域都比最聪明的人类大脑都聪明很多，包括科学创新、通识和社交技能。”超人工智能可以是各方面都比人类强一点，也可以是各方面都比人类强万亿倍的。

　　我们现在处于一个充满弱人工智能的世界，比如垃圾邮件分类系统，是一个可以帮助我们筛选垃圾邮件的弱人工智能；Google翻译是一个可以帮助我们翻译英文的弱人工智能；AlphaGo是一个可以战胜世界围棋冠军的弱人工智能等等。这些弱人工智能算法不断地加强创新，每一个弱人工智能的创新，都是在给通往强人工智能和超人工智能的旅途添砖加瓦。正如人工智能科学家Aaron Saenz所说，现在的弱人工智能就像地球早期软泥中的氨基酸，可能突然之间就形成了生命。

## 1.2 数据挖掘、机器学习与深度学习

　　大数据的兴起使得数据科学家作为一种新生职业被提出，数据研究高级科学家Rachel Schutt将其定义为“计算机科学家、软件工程师和统计学家的混合体”。数据挖掘作为一个学术领域，横跨多个学科，涵盖但不限于统计学、数学、机器学习和数据库，此外还运用在各类专业领域，比如油田、电力、海洋生物、历史文本、图像、电子通信等。

### 1.2.1 数据挖掘

　　简单来说，数据挖掘就是在大型的数据库中发现有用的信息，并加以分析的过程,也就是人们所说的KDD ( knowledge discovery in database )。一个数据的处理过程，就是从输入数据开始，对数据进行预处理，包括特征选择、规范化、降低维数、数据提升等，然后进行数据的分析和挖掘，再经过处理，例如模式识别、可视化等，最后形成可用信息的全过程。

　　所以说数据挖掘只是一种概念，即从数据中挖掘到有意义的信息，从大量的数据中寻找数据之间的特性。

### 1.2.2 机器学习

　　机器学习算是实现人工智能的一种途径，它和数据挖掘有一定的相似性，也是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。对比于数据挖掘从大数据之间找相互特性而言，机器学习更加注重算法的设计，让计算机能够自动地从数据中“学习”规律，并利用规律对未知数据进行预测。因为学习算法涉及了大量的统计学理论，与统计推断联系尤为紧密，所以也被称为统计学习方法。

　　机器学习可以分为以下五个大类：

　　 **（1）监督学习：**从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果二监督学习的训练集要求是输入和输出.也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归与分类。

　　**（2）无监督学习：**无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类等。

　　**（3）半监督学习：**这是一种介于监督学习与无监督学习之间的方法。

　　**（4） 迁移学习：**将已经训练好的模型参数迁移到新的模型来帮助新模型训练数据集。

　　**（5）增强学习：**通过观察周围环境来学习二每个动作都会对环境有所影响.学习对象根据观察到的周围环境的反馈来做出判断。

　　传统的机器学习算法有以下几种：线性回归模型、logistic回归模型、$k$近邻算法、决策树、随机森林、支持向量机、人工神经网络、EM算法、概率图模型等。

### 1.2.3 深度学习

　　深度学习的最初版本是人工神经网络，这是机器学习的一个分支，其试图模型人脑，通过更加复杂的结构自动提取数据特征。

　　在深度学习发展起来之前，机器学习虽然发展了几十年，但是还是存在很多人工智能没法解决的问题，例如图像识别、语音识别、自然语言处理等，而深度学习的出现则很好的解决了这些领域的一些问题。正因为高性能GPU的出现，使得训练复杂的网络模型成为可能，这也促进深度学习迅速发展。

　　首先通过一张图来概括一下深度学习的历史浪潮，如图1.1所示。

![图1.1 深度学习的发展史](http://ww1.sinaimg.cn/large/6deb72a3ly1g1xe9b7oxfj20fw0avabq.jpg)

<p align=center>图1.1 深度学习的发展史</p>

　　通过这张图，我们明显看出来深度学习经历了两个低谷，这两个低谷也将深度学习的发展分为了三个不同的阶段，下面分别讲述这三段历史。

　　**第一代神经网络（1958年-1969年）**

　　最早的神经网络的思想起源于1943年的MP人工神经元模型，当时人们希望能够用计算机来模拟人的神经元反应过程，该模型把神经元简化为三个过程：输入信号线性加权，求和，非线性激活（阈值法），如图1.2所示。

![](http://ww1.sinaimg.cn/large/6deb72a3ly1g1xeetrpfdj20jt06iq4d.jpg)

<p align=center>MP神经元模型</p>

　　第一次将MP用于机器学习（分类）的当属1958年Rosenblatt发明的感知器（perceptron）算法。该算法使用MP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，它的理论与实践效果引发了第一次神经网络的浪潮。

　　然而学科发展的历史不总是一帆风顺的。

　　1969年，美国数学家及人工智能先驱Minsky在其著作屮证明了感知器本质上是一种线性模型.只能处理线性分类问题.就连最简爪的XOR （亦或）问题都无法止确分类。这等于直接判了感知器的死刑。神经网络的研究也陷入了近20年的停滞。

　　**第二代神经网络（1986年一 1998年）**

　　笫一次打破推线性诅咒的是现代深度学习大牛Hinton。他在1986年发明了适用于多层感知器（MLP）的BP算法。并采用Sigmoid进行推线性映射。e有效解决了推线性分类和学习的问题。该方法引发了神经网络的第二次热潮。

　　1989年，Robert Hecht-Nielsen证明了 MLP的万能逼近定理。即对于任何闭区间内的一个连续函数$f$，都可以用含有一个隐含层的BP网络来逼近。该定理的发现，极大地鼓舞了神经网络的研究人员。

　　同样在1989年，LeCun发明了卷积神经网络——LeNet。并将其用于数字识别，取得了较好的成绩，不过当时并没有引超足够的注意。

　　值得强调的是因为1989年以后研究人员没有提出特别优秀的方法，且神经网络一直缺少相应的严格的数学理论支持，神经网络的热潮渐渐冷下去了。冰点发生于1991年，BP算法被指出存在梯度消失问题，即在误差梯度后向传递的过程中，后层梯度以乘性方式桑加到前层，由于Sigmoid函数的饱和特性，后层梯度本來就小，误差梯度传到前层时几乎为0。因此无法对前层进行有效的学习。该发现对此时的神经网络发展雪上加霜。

　　1997年，LSTM模型被发明，尽管该模型在序列建模上表现出的特性非常突岀，但由于正处于神经网络的下坡期，也没有引起足够的重视。

　　**统计学习方法的春天（1986年一2006年）**

　　1986年，决策树方法被提出，很快ID3, ID4, CART^改进的决策树方法相继出现（到目前仍然是非常常用的机器学习方法），这些方法也是符号学习方法的代表。正是由于这些方法的出现，使得统计学习开始进入人们的视野，迎来统计学习方法的春天。

　　1995年，统计学家Vapnik提出线性SVM。由于它有非常完美的数学理论推导做支撑（统计学与凸优化等），并且非常符合人的直观感受（最大间隔），逐渐成为当时的主流算法，更重要的是它在线性分类的问题上取得了当时最好的成绩，这使得神经网络更陷入无人问津的境地。

　　1997年，AdaBoost被提岀，该方法是PAC （Probably Approximately Correct）理论在机器学习实践上的代表，也促成了集成学习（Ensemble Learning）这一类方法的诞生，在回归和分类任务上取得了非常好的效果。该方法通过一系列的弱分类器集成，达到强分类器的效果。现在集成学习仍然活跃在传统机器学习方法中，在很多比赛中大放异彩。

　　2000年，KernelSVM被提出，核化的SVM通过一种巧妙的方式将原空间线性不可分的问题，通过Kernel映射成高维空间的线性可分问题，成功解决了非线性分类的问题，且分类效果非常好。至此也更加终结了神经网络时代，在有着如此多完美理论支持的方法中，神经网络似乎被宣告了死刑。

　　2001年，随机森林被提出，这是集成方法的另一代表，该方法的理论扎实，比AdaBoost能更好地抑制过拟合问题，实际效果也非常不错，使得机器学习更向前迈进一步。

　　2001年，一种新的统一框架一一图模型被提出，该方法试图统一机器学习混乱的方法，如朴素贝叶斯、SVM、隐马尔可夫模型等，为各种学习方法提供一个统一的描述框架，希望实现大一统的理论框架。

　　**第三代神经网络——深度学习（2006年至今）**

　　该阶段又分为两个时期：快速发展期（2006年-2012年）与爆发期（2012年至今）。

- **快速发展期（2006年一2012年）**

  　　　　2006年，深度学习元年。这一年，Hinton提出了深层网络训练中梯度消失问题的解决方案：“无监督预训练对权值进行初始化+有监督训练微调”。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。但是由于没有特别有效的实验验证，该论文并没有引起重视。

    　　2011年，ReLU激活函数被提出，该激活函数能够有效地抑制梯度消失的问题。

    　　2011年，微软首次将深度学习应用在语音识别上，取得了重大突破。

- **爆发期（2012年至今）**

   　　　2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，通过构建CNN网络AlexNet夺得冠军，并且碾压了第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引了众多研究者的注意。AlexNet的创新点主要有以下几点: （1）首次采用ReLU激活函数，极大加快了收敛速度，并且从根本上解决了梯度消失的问题；（2）  由于ReLU算法可以很好地抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全釆用有监督训练。也正因为如此，现在深度学习的主流学习方法也变成了纯粹的有监督学习；（3）  扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合；（4） 首次采用GPU对计算进行加速。

    　　2013年、2014年、2015年，通过ImageNet图像识别比赛，深度学习因其网络结构、训练方法、GPU硬件不断进步，在其他领域也不断地征服战场。

    　　2015年，Hinton, LeCun, Bengio论证了局部极值问题对于深度学习的影响，得到的结果是Loss的局部极值问题对于深层网络的影响可以忽略，该论断也消除了笼罩在神经网络上的局部极值问题的阴霾。具体原因是深层网络虽然局部极值非常多，但是通过深度学习的Batch Gradient Descent优化方法很难陷进去，而且就算陷进去，其局部极小值点与全局极小值点也是非常接近的。而浅层网络却虽然拥有较少的局部极小值点，但是却很容易陷进去，且这些局部极小值点与全局极小值点相差较大。对于这一点，论述原文其实没有严格地证明，只是简单叙述。

    　　2015年，何恺明提出Deep Residual Net。分层预训练时，ReLU和Batch Normalization都是为了解决深度神经网络优化时的梯度消失或者爆炸问题，但是在对更深层的神经网络进行优化时，又出现了新的Degradation问题，“即通常来说，如果在VGG16后面加上若干个单位映射，网络的输出特性将和VGG16一样，这说明更深层的网络的潜在分类性能只可能大于等于VGG16的性能，不可能变坏，然而实际效果却是：如果简单地加深VGG16的话，分类性能就会下降（不考虑模型过拟合问题）”。Residual网络认为这说明深度学习网络在学习单位映射方面有困难，因此设计了一个对于单位映射（或接近单位映射）有较强学习能力的深度学习网络，极大地增强了深度学习网络的表达能力。使用此方法能够轻松地训练高达150层的网络，使得深度学习成为了名副其实的“深度”学习。

    　　随着神经网络的发展，目前比较流行的网络结构分别有：深度神经网络（DNN）、卷积神经网络（CNN）、循坏递归神经网络（RNN）、生成对抗网络 （GAN）等等，本书会逐一详细介绍，在此不再赘述。

## 1.3 学习资源与建议

　　随着近年深度学习的兴起，很多研究者都投入这个领域当中，由于各个大学都将自己的课程放到了网上，出现了很多学习资源和网络课程，而且很多大公司如Google和 Facebook都将自己的开源框架放到了Github上，使得深度学习的入门越来越简单，很多麻烦的重复操作已经通过框架简化了，这也让我们每个人都能有机会接触深度学习。

　　网上有各种各样的学习经验分享，有的人注重理论知识的积累，看了很多书，但是动手实践的经验为0；也有一些人热衷于代码的实现，每天学习别人已经写好的代码。对于这两种情况，我认为都是不好的，深度学习是理论和工程相结合的领域，不仅仅需要写代码的能力强，也需要有理论知识能够看懂论文，实现论文提出来的新想法，所以我们的学习路线应该是理论与代码相结合，平衡两边的学习任务，不能出现只管一边而不学另外一边的情况，因为只有理论与代码兼顾才不至于一旦学习深入，就会发现自己会有很多知识的漏洞。

　　在学习本书之前，需要一定的Python语言基础和微积分、线性代数的基础，下面将给出开始阅读本书之前的学习建议，以及读完本书后继续深入了解深度学习领域的学习建议。

　　在开始本书之前，对于微积分和线性代数需要掌握的知识并不多，对于微积分只需要知道导数和偏导数，对于线性导数只需要知道矩阵乘法就可以了。

　　对于Python语言，可以学习廖雪峰的python教程，地址：https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000，面向零基础的初学者，通过简单例子快速入门Python的基本操作。

　　同时，给出一些参考学习资源：

　　（1）机器学习基础：由易到难排列为：Coursera上Andrew Ng的机器学习课程、周志华著的《机器学习》、李航著的《统计学习方法》、Pattern Recognition and Machine Learning

　　（2）深度学习：Udacity的两个深度学习课程、Coursera的Neural Networks for Machine Learning、Standford的cs231n，cs224n。
　　





